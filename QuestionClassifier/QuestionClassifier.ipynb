{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9840425531914894\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Factual       0.96      1.00      0.98        52\n",
      "    Multimedia       0.98      1.00      0.99        42\n",
      "Recommendation       1.00      1.00      1.00        42\n",
      "     Unrelated       1.00      0.94      0.97        52\n",
      "\n",
      "      accuracy                           0.98       188\n",
      "     macro avg       0.98      0.99      0.98       188\n",
      "  weighted avg       0.98      0.98      0.98       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv(\"Movie_Questions_Base.csv\")\n",
    "\n",
    "# Define the features (questions) and labels (categories)\n",
    "X = data['question']\n",
    "y = data['category']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save the trained SVM model and vectorizer\n",
    "torch.save({\n",
    "    'svm_model': svm_model,\n",
    "    'vectorizer': vectorizer\n",
    "}, \"svm_question_classifier.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangy\\AppData\\Local\\Temp\\ipykernel_27520\\4068554469.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"svm_question_classifier.pth\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the model and vectorizer\n",
    "checkpoint = torch.load(\"svm_question_classifier.pth\")\n",
    "loaded_model = checkpoint['svm_model']\n",
    "loaded_vectorizer = checkpoint['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unrelated']\n"
     ]
    }
   ],
   "source": [
    "# Use the loaded model for inference\n",
    "new_questions = [\"Show me Cillian Murphy\"]\n",
    "new_questions_tfidf = loaded_vectorizer.transform(new_questions)\n",
    "predictions = loaded_model.predict(new_questions_tfidf)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "EMBEDDING_REL_MAPPING = {\n",
    "    \"director\": [\"director\", \"directed\", \"directs\", \"direct\"],\n",
    "    \"award\": [\"award\", \"oscar\", \"prize\"],\n",
    "    'publication date': ['release', 'date', 'released', 'releases','release date', 'publication', 'launch', 'broadcast','launched'],\n",
    "    'executive producer': ['showrunner', 'executive producer'],\n",
    "    'screenwriter': ['screenwriter', 'scriptwriter', 'writer', 'story'],\n",
    "    'film editor': ['editor', 'film editor'],\n",
    "    'box office': ['box', 'office', 'funding', 'box office'],\n",
    "    'cost': ['budget', 'cost'],\n",
    "    'nominated for': ['nomination', 'award', 'finalist', 'shortlist', 'selection', 'nominated for'],\n",
    "    'production company': ['company', 'company of production', \"produced\", 'production company'],\n",
    "    'country of origin': ['origin', 'country', 'country of origin'],\n",
    "    'cast member' :['actor', 'actress', 'cast', 'cast member'],\n",
    "    'genre': ['type', 'kind', 'genre'],\n",
    "}\n",
    "\n",
    "\n",
    "class QuestionType(Enum):\n",
    "    FACTUAL = \"Factual\"\n",
    "    RECOMMENDATION = \"Recommendation\"\n",
    "    MULTIMEDIA = \"Multimedia\"\n",
    "    UNRELATED = \"Unrelated\"\n",
    "    \n",
    "def _get_question_type(user_query) -> QuestionType:\n",
    "\n",
    "    fall_back_type = QuestionType.FACTUAL\n",
    "\n",
    "    # Use the loaded model for inference\n",
    "    new_questions = [user_query]\n",
    "    new_questions_tfidf = loaded_vectorizer.transform(new_questions)\n",
    "    predictions = loaded_model.predict(new_questions_tfidf)\n",
    "    \n",
    "    if not predictions:\n",
    "        return fall_back_type\n",
    "    \n",
    "    type = predictions[0]\n",
    "    \n",
    "    match type:\n",
    "        case \"Factual\": return QuestionType.FACTUAL\n",
    "        case \"Recommendation\": return QuestionType.RECOMMENDATION\n",
    "        case \"Multimedia\": return QuestionType.MULTIMEDIA\n",
    "        case \"Unrelated\": return _double_check_question_type_for_unRelated(user_query)\n",
    "\n",
    "def _double_check_question_type_for_unRelated(user_query) -> QuestionType:\n",
    "    user_query = re.sub(r'[^a-zA-Z0-9 ]', '', user_query.lower().strip())\n",
    "\n",
    "    factual_keywords = {\"language\", \"mpaa\"}\n",
    "    for key, keywords in EMBEDDING_REL_MAPPING.items():\n",
    "        factual_keywords.update(keywords)\n",
    "\n",
    "    # Keywords for multimedia-related queries\n",
    "    multimedia_keywords = {\"show\", \"display\", \"view\", \"present\", \"see\"}\n",
    "    # Keywords for recommendation-related queries\n",
    "    recommendation_keywords = {\"recommend\", \"suggest\", \"advise\", \"offer\", \"favor\", \"i like\", \"i like\"}\n",
    "    \n",
    "    if any(keyword in user_query for keyword in factual_keywords):\n",
    "        return QuestionType.FACTUAL\n",
    "    \n",
    "    if any(keyword in user_query for keyword in multimedia_keywords):\n",
    "        return QuestionType.MULTIMEDIA\n",
    "    \n",
    "    if any(keyword in user_query for keyword in recommendation_keywords):\n",
    "        return QuestionType.RECOMMENDATION\n",
    "\n",
    "    return QuestionType.UNRELATED\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unrelated']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<QuestionType.FACTUAL: 'Factual'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the loaded model for inference\n",
    "user_query = \"Who is the executive producer of X-Men: First Class? \"\n",
    "new_questions = [user_query]\n",
    "new_questions_tfidf = loaded_vectorizer.transform(new_questions)\n",
    "predictions = loaded_model.predict(new_questions_tfidf)\n",
    "print(predictions)\n",
    "_get_question_type(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATAIChatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
