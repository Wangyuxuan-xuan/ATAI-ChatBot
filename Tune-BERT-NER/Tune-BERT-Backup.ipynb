{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import rdflib\n",
    "\n",
    "# graph = rdflib.Graph()\n",
    "# serialized_path = \"../Dataset/graph.pkl\"\n",
    "# with open(serialized_path, 'rb') as f:\n",
    "#     print(\"Loading serialized graph\")\n",
    "#     graph = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_template = '''\n",
    "#             PREFIX ddis: <http://ddis.ch/atai/>\n",
    "#             PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "#             PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "#             PREFIX schema: <http://schema.org/>\n",
    "#             PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            \n",
    "#             SELECT ?movie ?movieLabel ?predicateLabel ?object ?objectLabel WHERE {{\n",
    "#                 # Find the movie entity based on an exact match for the label\n",
    "#                 ?movie rdfs:label \"{0}\"@en .\n",
    "                \n",
    "#                 # Retrieve all predicates and objects related to the movie entity\n",
    "#                 ?movie ?predicate ?object .\n",
    "\n",
    "#                 FILTER(?predicate IN (\n",
    "#                       wdt:P31,   # instance of\n",
    "#                       wdt:P57,   # director\n",
    "#                       wdt:P162,  # producer\n",
    "#                       wdt:P364,  # original language\n",
    "#                       wdt:P272,  # production company\n",
    "#                       wdt:P58,   # screenwriter\n",
    "#                       wdt:P166,  # award received\n",
    "#                       wdt:P2047, # duration\n",
    "#                       wdt:P577 # release date\n",
    "#                   ))\n",
    "\n",
    "#                 # Optionally retrieve labels for predicates and objects\n",
    "#                 OPTIONAL {{ ?predicate rdfs:label ?predicateLabel . FILTER(LANG(?predicateLabel) = \"en\") }}\n",
    "#                 OPTIONAL {{ ?object rdfs:label ?objectLabel . FILTER(LANG(?objectLabel) = \"en\") }}\n",
    "#                 OPTIONAL {{ ?movie rdfs:label ?movieLabel . FILTER(LANG(?movieLabel) = \"en\") }}\n",
    "#             }}\n",
    "#             ORDER BY ?movie\n",
    "#         '''\n",
    "\n",
    "# movie_name = \"The Godfather\"\n",
    "# query = query_template.format(movie_name)\n",
    "\n",
    "# result = graph.query(query)\n",
    "# res = [str(row) for row in result]\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Query All Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the SPARQL query\n",
    "# query = '''\n",
    "# PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "# PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "# PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "# SELECT ?movieLabel WHERE {\n",
    "#   ?movie wdt:P31 wd:Q11424 .\n",
    "#   ?movie rdfs:label ?movieLabel .\n",
    "#   FILTER(LANG(?movieLabel) = \"en\")\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# # Execute the query\n",
    "# result = graph.query(query)\n",
    "\n",
    "# # Extract movie names\n",
    "# movie_names = [str(row.movieLabel) for row in result]\n",
    "# print(\"Total movies found:\", len(movie_names))\n",
    "# print(movie_names)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Dataset/MovieTitles\", 'wb') as f:\n",
    "#     pickle.dump(movie_names, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Query all person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the SPARQL query for extracting persons\n",
    "# query = '''\n",
    "# PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "# PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "# PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "# SELECT ?personLabel WHERE {\n",
    "#   ?person wdt:P31 wd:Q5 .  # Q5 represents humans on Wikidata\n",
    "#   ?person rdfs:label ?personLabel .\n",
    "#   FILTER(LANG(?personLabel) = \"en\")\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# # Execute the query and extract person names\n",
    "# result = graph.query(query)\n",
    "# person_names = [str(row.personLabel) for row in result]\n",
    "# print(\"Total persons found:\", len(person_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Dataset/MovieTitles\", 'wb') as f:\n",
    "#     pickle.dump(person_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Read movie titles from the text file and save as a pickle file\n",
    "with open(\"../Dataset/MovieTitles.txt\", 'r') as txt_file:\n",
    "    # Read the content of the text file\n",
    "    content = txt_file.read()\n",
    "    \n",
    "    # Convert the string representation of a list into a Python list\n",
    "    movie_titles = eval(content)\n",
    "\n",
    "# Save the movie titles as a pickle file\n",
    "with open(\"../Dataset/MovieTitles.pickle\", 'wb') as pickle_file:\n",
    "    pickle.dump(movie_titles, pickle_file)\n",
    "\n",
    "# Load movie titles from the pickle file\n",
    "with open(\"../Dataset/MovieTitles.pickle\", 'rb') as pickle_file:\n",
    "    movie_titles = pickle.load(pickle_file)\n",
    "\n",
    "# Convert movie titles to a list (in case it is not already)\n",
    "movie_titles = list(movie_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Templates where \"{MOVIE}\" will be replaced with an actual movie title\n",
    "templates_unused = [\n",
    "\"Who directed the movie {MOVIE}?\",\n",
    "\"What is the release date of {MOVIE}?\",\n",
    "\"What was {MOVIE} released?\",\n",
    "\"Who played the lead role in {MOVIE}?\",\n",
    "\"Did {MOVIE} win any Academy Awards?\",\n",
    "\"What is the IMDb rating of {MOVIE}?\",\n",
    "\"What is genre of {MOVIE}?\",\n",
    "\"Who wrote the screenplay for {MOVIE}?\",\n",
    "\"Who is the screenwriter of {MOVIE}?\",\n",
    "\"What is the box office collection of {MOVIE}?\",\n",
    "\"What genre is {MOVIE}?\",\n",
    "\"Who was the cinematographer for {MOVIE}?\",\n",
    "\"What is the runtime of {MOVIE}?\",\n",
    "\"Did {MOVIE} feature any award-winning performances?\",\n",
    "\"Who composed the music for {MOVIE}?\",\n",
    "\"What is the main theme of {MOVIE}?\",\n",
    "\"What award did {MOVIE} recive\",\n",
    "\"What is the production company of {MOVIE}\"\n",
    "\"What language does {MOVIE} support\"\n",
    "\"Is {MOVIE} based on a true story?\",\n",
    "\"When was {MOVIE} released?\",\n",
    "\"Who directed {MOVIE}?\",\n",
    "\"film editor of {MOVIE}\"\n",
    "\"Who is the director of the movie {MOVIE}?\",\n",
    "\"Who is the writer of {MOVIE}?\",\n",
    "\"Is {MOVIE} set in a specific historical period?\",\n",
    "\"Who is the executive producer of {MOVIE}?\",\n",
    "\"I loved {MOVIE} and {MOVIE}. What else should I watch?\",\n",
    "\"What movies will I like if I like {MOVIE}?\",\n",
    "\"Can you recommend movies similar with {MOVIE}?\",\n",
    "\"Recommend movies similar to {MOVIE}.\",\n",
    "\"What movies should I watch if I loved {MOVIE}?\",\n",
    "\"Given that I like {MOVIE}, {MOVIE}, and {MOVIE}, recommend some movies.\",\n",
    "\"Suggest films with similar vibes to {MOVIE}.\",\n",
    "\"Recommend movies like {MOVIE}.\",\n",
    "\"Recommend movies with time travel themes, for example {MOVIE}.\",\n",
    "\"Suggest movies related to {MOVIE}.\",\n",
    "\"What movies are like {MOVIE}?\",\n",
    "\"Any recommendations for animated movies like {MOVIE}?\",\n",
    "\"What films are similar to {MOVIE}.\",\n",
    "\"Recommend movies that are similar to {MOVIE}.\",\n",
    "\"Suggest movies directed by {SOME_NAME}.\",\n",
    "\"What movies should I watch if I enjoyed {MOVIE}?\",\n",
    "\"Recommend animated movies to watch if I liked {MOVIE}.\"\n",
    "]\n",
    "\n",
    "\n",
    "templates = [\n",
    "\n",
    "    \"When was {MOVIE} released?\"\n",
    "    \"Who directed the movie {MOVIE}?\"\n",
    "    \"Who directed {MOVIE}?\"\n",
    "    \"Who is the director of {MOVIE}?\"\n",
    "    \"Did James Cameron direct {MOVIE}?\"\n",
    "    \"Does James Cameron direct {MOVIE}?\"\n",
    "    \"Did {MOVIE} have the same director of {MOVIE}?\"\n",
    "    \"Is {MOVIE} set in the French Renaissance period?\",\n",
    "    \"Is {MOVIE} directed by Stanley Kubrick?\",\n",
    "    \"Is {MOVIE} a sequel?\",\n",
    "    \"Recommend movies similar to {MOVIE} and {MOVIE}. \"\n",
    "    \"Recommend movies like {MOVIE}.\",\n",
    "    \"Recommend movies like {MOVIE}, {MOVIE}, and {MOVIE}. \"\n",
    "    \"Given that I like {MOVIE}, {MOVIE}, and {MOVIE}, recommend some movies.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to movie_title_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_FILE = \"movie_title_dataset.csv\"\n",
    "\n",
    "# Generate annotated sentences\n",
    "def generate_dataset(movie_titles, templates, size=80000, output_file=DATASET_FILE):\n",
    "    dataset = []\n",
    "\n",
    "    SENTENCE_LEN_THRESHOLD = 100\n",
    "    # Generate random combinations of templates and movie titles\n",
    "    while len(dataset) < size:\n",
    "        template = random.choice(templates)\n",
    "\n",
    "        # Count number of {MOVIE} placeholders\n",
    "        movie_placeholders = template.count(\"{MOVIE}\")\n",
    "\n",
    "        # Select distinct movie titles for each placeholder\n",
    "        selected_movies = random.sample(movie_titles, movie_placeholders)\n",
    "\n",
    "        # Replace each {MOVIE} with a distinct movie title\n",
    "        sentence = template\n",
    "        for movie in selected_movies:\n",
    "            sentence = sentence.replace(\"{MOVIE}\", movie, 1)\n",
    "\n",
    "        if len(sentence) > SENTENCE_LEN_THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        # Create annotation in the desired format\n",
    "        annotated_sentence = []\n",
    "        for word in sentence.split():\n",
    "            if any(word == m.split()[0] for m in selected_movies):  # Start of movie title\n",
    "                annotated_sentence.append((word, \"B-MOVIE\"))\n",
    "            elif any(word in m.split() for m in selected_movies):\n",
    "                annotated_sentence.append((word, \"I-MOVIE\"))\n",
    "            else:\n",
    "                annotated_sentence.append((word, \"O\"))\n",
    "\n",
    "        # Store the sentence and its annotations\n",
    "        words, tags = zip(*annotated_sentence)\n",
    "        dataset.append({\"sentence\": \" \".join(words), \"labels\": \" \".join(tags)})\n",
    "\n",
    "    # Save dataset as CSV\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"sentence\", \"labels\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dataset)\n",
    "\n",
    "    print(f\"Dataset saved to {output_file}\")\n",
    "\n",
    "# Generate the training data\n",
    "TRAIN_DATA = generate_dataset(movie_titles, templates)\n",
    "# print(TRAIN_DATA[:5])  # Print first 5 examples for preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "DATASET_FILE = \"movie_title_dataset.csv\"\n",
    "# Load dataset from CSV\n",
    "def load_dataset(input_file=DATASET_FILE):\n",
    "    return pd.read_csv(input_file)\n",
    "\n",
    "# Load and split the dataset\n",
    "dataset = load_dataset(DATASET_FILE)\n",
    "train_data, eval_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "eval_dataset = Dataset.from_pandas(eval_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "tokenize the sentences in the dataset using the `BertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Movie labels\n",
    "label_map = {\"O\": 0, \"B-MOVIE\": 1, \"I-MOVIE\": 2}\n",
    "\n",
    "# Tokenize function with label mapping\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])\n",
    "            else:\n",
    "                # Assign -100 to subword tokens to ignore them in loss computation\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization to your Hugging Face dataset\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Tune the BERT-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U accelerate\n",
    "# %pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should display your GPU name, e.g., 'NVIDIA GeForce RTX 4060'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import os\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='logs/training.log',  # Specify the log file path\n",
    "    filemode='w',  # 'w' for overwriting each time, 'a' for appending\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO  # Log level can be adjusted (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    ")\n",
    "\n",
    "\n",
    "# Example log to verify\n",
    "logging.info(\"Logging setup is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=len(label_map))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save the model at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# Trainer initialization (same as before)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "# Train the model \n",
    "try:\n",
    "    trainer.train()\n",
    "    logging.info(\"Training completed successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during training: {e}\")\n",
    "\n",
    "logging.info(\"Training process has ended.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_ner_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_ner_model\")\n",
    "\n",
    "print(\"Model and tokenizer saved to './fine_tuned_ner_model'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2efd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wangy\\anaconda3\\envs\\ATAIChatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \"Let's talk about Avatar.\"\n",
      "\n",
      "Sentence: \"When was The Godfather released?\"\n",
      "  - Entity: 'the', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'godfather', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"When was vampire assassin released?\"\n",
      "  - Entity: 'vampire', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'assassin', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Who is the screenwriter of The Masked Gang: Cyprus?\"\n",
      "  - Entity: 'who is the screenwriter of the masked gang :', Label: 'LABEL_2', Confidence: 0.96\n",
      "\n",
      "Sentence: \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\"\n",
      "  - Entity: 'who', Label: 'LABEL_1', Confidence: 0.91\n",
      "  - Entity: 'is the director of star wars : episode vi - return of', Label: 'LABEL_2', Confidence: 0.99\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./fine_tuned_ner_model\", tokenizer=\"./fine_tuned_ner_model\", aggregation_strategy=\"simple\", device=\"cuda\")\n",
    "\n",
    "# Example inference\n",
    "sentences = [\n",
    "    \"Let's talk about Avatar.\",\n",
    "    \"When was The Godfather released?\",\n",
    "    \"When was vampire assassin released?\",\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "]\n",
    "\n",
    "sentences_1 = [\n",
    "        \"Did Christopher Nolan direct Inception?\",\n",
    "    \"Is GoldenEye 007 a James Bond movie?\",\n",
    "\"Is Following a black and white film?\",\n",
    "\"Does the lord of the Rings Trilogy consist of three movies?\",\n",
    "\"Does First Man depict the life of Neil Armstrong?\",\n",
    "\"Is La Princesse de Clèves set in the French Renaissance period?\",\n",
    "\"Is 2001: A Space Odyssey directed by Stanley Kubrick?\",\n",
    "\"Is Devil in the Flesh 2 a sequel?\",\n",
    "\"Did James Cameron direct Titanic?\",\n",
    "]\n",
    "\n",
    "recommendation_sentence = [\n",
    "    \"Given that I like Inception, The Godfather can you recommend me some movies\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    ner_results = ner_pipeline(s)\n",
    "\n",
    "    print(f\"\\nSentence: \\\"{s}\\\"\")\n",
    "    if ner_results:\n",
    "        for entity in ner_results:\n",
    "            label = entity[\"entity_group\"]\n",
    "            word = entity[\"word\"]\n",
    "            score = entity[\"score\"]\n",
    "            if label in ('LABEL_1', 'LABEL_2'):\n",
    "                print(f\"  - Entity: '{word}', Label: '{label}', Confidence: {score:.2f}\")\n",
    "    else:\n",
    "        print(\"No entities found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \"Given that I like Inception, The Godfather can you recommend me some movies\"\n",
      "  - Entity: 'I', Label: 'LABEL_1', Confidence: 0.89\n",
      "  - Entity: 'like Inception, The Godfather', Label: 'LABEL_2', Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./Tuned_BERT_NER_movie-60000\", tokenizer=\"./Tuned_BERT_NER_movie-60000\", aggregation_strategy=\"simple\", device=\"cuda\")\n",
    "\n",
    "# Example inference\n",
    "sentences = [\n",
    "    \"Let's talk about Avatar.\",\n",
    "    \"When was The Godfather released?\",\n",
    "    \"When was vampire assassin released?\",\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "]\n",
    "\n",
    "sentences_1 = [\n",
    "        \"Did Christopher Nolan direct Inception?\",\n",
    "    \"Is GoldenEye 007 a James Bond movie?\",\n",
    "\"Is Following a black and white film?\",\n",
    "\"Does the lord of the Rings Trilogy consist of three movies?\",\n",
    "\"Does First Man depict the life of Neil Armstrong?\",\n",
    "\"Is La Princesse de Clèves set in the French Renaissance period?\",\n",
    "\"Is 2001: A Space Odyssey directed by Stanley Kubrick?\",\n",
    "\"Is Devil in the Flesh 2 a sequel?\",\n",
    "\"Did James Cameron direct Titanic?\",\n",
    "]\n",
    "\n",
    "recommendation_sentence = [\n",
    "    \"Given that I like Inception, The Godfather can you recommend me some movies\"\n",
    "]\n",
    "\n",
    "for s in recommendation_sentence:\n",
    "    ner_results = ner_pipeline(s)\n",
    "\n",
    "    print(f\"\\nSentence: \\\"{s}\\\"\")\n",
    "    if ner_results:\n",
    "        for entity in ner_results:\n",
    "            label = entity[\"entity_group\"]\n",
    "            word = entity[\"word\"]\n",
    "            score = entity[\"score\"]\n",
    "            if label in ('LABEL_1', 'LABEL_2'):\n",
    "                print(f\"  - Entity: '{word}', Label: '{label}', Confidence: {score:.2f}\")\n",
    "    else:\n",
    "        print(\"No entities found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATAIChatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
