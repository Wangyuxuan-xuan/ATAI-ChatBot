{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "71197b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import rdflib\n",
    "\n",
    "# graph = rdflib.Graph()\n",
    "# serialized_path = \"../Dataset/graph.pkl\"\n",
    "# with open(serialized_path, 'rb') as f:\n",
    "#     print(\"Loading serialized graph\")\n",
    "#     graph = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_template = '''\n",
    "#             PREFIX ddis: <http://ddis.ch/atai/>\n",
    "#             PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "#             PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "#             PREFIX schema: <http://schema.org/>\n",
    "#             PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "            \n",
    "#             SELECT ?movie ?movieLabel ?predicateLabel ?object ?objectLabel WHERE {{\n",
    "#                 # Find the movie entity based on an exact match for the label\n",
    "#                 ?movie rdfs:label \"{0}\"@en .\n",
    "                \n",
    "#                 # Retrieve all predicates and objects related to the movie entity\n",
    "#                 ?movie ?predicate ?object .\n",
    "\n",
    "#                 FILTER(?predicate IN (\n",
    "#                       wdt:P31,   # instance of\n",
    "#                       wdt:P57,   # director\n",
    "#                       wdt:P162,  # producer\n",
    "#                       wdt:P364,  # original language\n",
    "#                       wdt:P272,  # production company\n",
    "#                       wdt:P58,   # screenwriter\n",
    "#                       wdt:P166,  # award received\n",
    "#                       wdt:P2047, # duration\n",
    "#                       wdt:P577 # release date\n",
    "#                   ))\n",
    "\n",
    "#                 # Optionally retrieve labels for predicates and objects\n",
    "#                 OPTIONAL {{ ?predicate rdfs:label ?predicateLabel . FILTER(LANG(?predicateLabel) = \"en\") }}\n",
    "#                 OPTIONAL {{ ?object rdfs:label ?objectLabel . FILTER(LANG(?objectLabel) = \"en\") }}\n",
    "#                 OPTIONAL {{ ?movie rdfs:label ?movieLabel . FILTER(LANG(?movieLabel) = \"en\") }}\n",
    "#             }}\n",
    "#             ORDER BY ?movie\n",
    "#         '''\n",
    "\n",
    "# movie_name = \"The Godfather\"\n",
    "# query = query_template.format(movie_name)\n",
    "\n",
    "# result = graph.query(query)\n",
    "# res = [str(row) for row in result]\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Query All Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the SPARQL query\n",
    "# query = '''\n",
    "# PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "# PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "# PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "# SELECT ?movieLabel WHERE {\n",
    "#   ?movie wdt:P31 wd:Q11424 .\n",
    "#   ?movie rdfs:label ?movieLabel .\n",
    "#   FILTER(LANG(?movieLabel) = \"en\")\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# # Execute the query\n",
    "# result = graph.query(query)\n",
    "\n",
    "# # Extract movie names\n",
    "# movie_names = [str(row.movieLabel) for row in result]\n",
    "# print(\"Total movies found:\", len(movie_names))\n",
    "# print(movie_names)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Dataset/MovieTitles\", 'wb') as f:\n",
    "#     pickle.dump(movie_names, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Query all person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the SPARQL query for extracting persons\n",
    "# query = '''\n",
    "# PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "# PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "# PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "# SELECT ?personLabel WHERE {\n",
    "#   ?person wdt:P31 wd:Q5 .  # Q5 represents humans on Wikidata\n",
    "#   ?person rdfs:label ?personLabel .\n",
    "#   FILTER(LANG(?personLabel) = \"en\")\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# # Execute the query and extract person names\n",
    "# result = graph.query(query)\n",
    "# person_names = [str(row.personLabel) for row in result]\n",
    "# print(\"Total persons found:\", len(person_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Dataset/MovieTitles\", 'wb') as f:\n",
    "#     pickle.dump(person_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Read movie titles from the text file and save as a pickle file\n",
    "with open(\"../Dataset/MovieTitles.txt\", 'r') as txt_file:\n",
    "    # Read the content of the text file\n",
    "    content = txt_file.read()\n",
    "    \n",
    "    # Convert the string representation of a list into a Python list\n",
    "    movie_titles = eval(content)\n",
    "\n",
    "# Save the movie titles as a pickle file\n",
    "with open(\"../Dataset/MovieTitles.pickle\", 'wb') as pickle_file:\n",
    "    pickle.dump(movie_titles, pickle_file)\n",
    "\n",
    "# Load movie titles from the pickle file\n",
    "with open(\"../Dataset/MovieTitles.pickle\", 'rb') as pickle_file:\n",
    "    movie_titles = pickle.load(pickle_file)\n",
    "\n",
    "# Convert movie titles to a list (in case it is not already)\n",
    "movie_titles = list(movie_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    # Existing templates\n",
    "    \"When was {MOVIE} released?\",\n",
    "    \"Who directed the movie {MOVIE}?\",\n",
    "    \"Who directed {MOVIE}?\",\n",
    "    \"Who is the director of {MOVIE}?\",\n",
    "    \"Did James Cameron direct {MOVIE}?\",\n",
    "    \"Does James Cameron direct {MOVIE}?\",\n",
    "    \"Did {MOVIE} have the same director as {MOVIE}?\",\n",
    "    \"Is {MOVIE} set in the French Renaissance period?\",\n",
    "    \"Is {MOVIE} directed by Stanley Kubrick?\",\n",
    "    \"Is {MOVIE} a sequel?\",\n",
    "    \"Recommend movies similar to {MOVIE} and {MOVIE}.\",\n",
    "    \"Recommend movies like {MOVIE}.\",\n",
    "    \"Recommend movies like {MOVIE}, {MOVIE}, and {MOVIE}.\",\n",
    "    \"Given that I like {MOVIE}, {MOVIE}, and {MOVIE}, recommend some movies.\",\n",
    "\n",
    "    # Additional templates\n",
    "    \"What is the release date of {MOVIE}?\",\n",
    "    \"What was {MOVIE} released?\",\n",
    "    \"Who played the lead role in {MOVIE}?\",\n",
    "    \"Did {MOVIE} win any Academy Awards?\",\n",
    "    \"What is the IMDb rating of {MOVIE}?\",\n",
    "    \"What is the genre of {MOVIE}?\",\n",
    "    \"Who wrote the screenplay for {MOVIE}?\",\n",
    "    \"Who is the screenwriter of {MOVIE}?\",\n",
    "    \"What is the box office collection of {MOVIE}?\",\n",
    "    \"Who was the cinematographer for {MOVIE}?\",\n",
    "    \"What is the runtime of {MOVIE}?\",\n",
    "    \"Did {MOVIE} feature any award-winning performances?\",\n",
    "    \"Who composed the music for {MOVIE}?\",\n",
    "    \"What is the main theme of {MOVIE}?\",\n",
    "    \"What awards did {MOVIE} receive?\",\n",
    "    \"What is the production company of {MOVIE}?\",\n",
    "    \"What languages does {MOVIE} support?\",\n",
    "    \"Is {MOVIE} based on a true story?\",\n",
    "    \"Who is the writer of {MOVIE}?\",\n",
    "    \"Is {MOVIE} set in a specific historical period?\",\n",
    "    \"Who is the executive producer of {MOVIE}?\",\n",
    "    \"I loved {MOVIE} and {MOVIE}. What else should I watch?\",\n",
    "    \"What movies will I like if I like {MOVIE}?\",\n",
    "    \"Can you recommend movies similar to {MOVIE}?\",\n",
    "    \"Given that I like {MOVIE}, {MOVIE}, and {MOVIE}, recommend some movies.\",\n",
    "    \"Suggest films with similar vibes to {MOVIE}.\",\n",
    "    \"Recommend movies with time travel themes, for example {MOVIE}.\",\n",
    "    \"Suggest movies related to {MOVIE}.\",\n",
    "    \"What movies are like {MOVIE}?\",\n",
    "    \"Any recommendations for animated movies like {MOVIE}?\",\n",
    "    \"Recommend movies that are similar to {MOVIE}.\",\n",
    "    \"What movies should I watch if I enjoyed {MOVIE}?\",\n",
    "    \"Recommend animated movies to watch if I liked {MOVIE}.\",\n",
    "    # Factual questions converted into templates\n",
    "    \"When was {MOVIE} released?\",\n",
    "    \"Who is the director of {MOVIE}?\",\n",
    "    \"Who directed {MOVIE}?\",\n",
    "    \"What is the MPAA film rating of {MOVIE}?\",\n",
    "    \"What is the genre of {MOVIE}?\",\n",
    "    \"What is the box office of {MOVIE}?\",\n",
    "    \"Can you tell me the publication date of {MOVIE}?\",\n",
    "    \"Who is the executive producer of {MOVIE}?\",\n",
    "    \"Which director is known for {MOVIE}?\",\n",
    "    \"Is {MOVIE} set in the French Renaissance period?\",\n",
    "    # Recommendation questions converted into templates\n",
    "    \"Recommend movies similar to {MOVIE} and {MOVIE}.\",\n",
    "    \"Given that I like {MOVIE}, {MOVIE}, and {MOVIE}, can you recommend some movies?\",\n",
    "    \"Recommend movies like {MOVIE}, {MOVIE}, and {MOVIE}.\",\n",
    "    \"What movies will I like if I like {MOVIE}?\",\n",
    "    \"Give me movies like {MOVIE}.\",\n",
    "]\n",
    "\n",
    "additional_templates = [\n",
    "    # General queries\n",
    "    \"Tell me about {MOVIE}.\",\n",
    "    \"What is {MOVIE} about?\",\n",
    "    \"Can you provide a summary of {MOVIE}?\",\n",
    "    \"Is {MOVIE} worth watching?\",\n",
    "    \"What are some reviews of {MOVIE}?\",\n",
    "    # Awards and nominations\n",
    "    \"How many awards has {MOVIE} won?\",\n",
    "    \"Was {MOVIE} nominated for any Oscars?\",\n",
    "    \"Did {MOVIE} win any Golden Globe awards?\",\n",
    "    # Cast and crew\n",
    "    \"Who starred in {MOVIE}?\",\n",
    "    \"Who are the main actors in {MOVIE}?\",\n",
    "    \"List the cast of {MOVIE}.\",\n",
    "    \"Who produced {MOVIE}?\",\n",
    "    # Sequels and series\n",
    "    \"Is {MOVIE} part of a series?\",\n",
    "    \"What is the sequel to {MOVIE}?\",\n",
    "    \"What movies are prequels to {MOVIE}?\",\n",
    "    # Release information\n",
    "    \"When did {MOVIE} come out?\",\n",
    "    \"What year was {MOVIE} released?\",\n",
    "    # Genre and style\n",
    "    \"Is {MOVIE} a comedy or a drama?\",\n",
    "    \"What style of film is {MOVIE}?\",\n",
    "    \"Is {MOVIE} a horror movie?\",\n",
    "    # Recommendations based on mood or theme\n",
    "    \"I'm looking for movies like {MOVIE}. Any suggestions?\",\n",
    "    \"What should I watch if I enjoyed {MOVIE}?\",\n",
    "    \"Movies similar in theme to {MOVIE}?\",\n",
    "    # Box office and ratings\n",
    "    \"How successful was {MOVIE} at the box office?\",\n",
    "    \"What ratings did {MOVIE} receive?\",\n",
    "    \"Is {MOVIE} critically acclaimed?\",\n",
    "    # Availability\n",
    "    \"Where can I watch {MOVIE}?\",\n",
    "    \"Is {MOVIE} available on Netflix?\",\n",
    "    # Personal opinions\n",
    "    \"Do you think {MOVIE} is a good film?\",\n",
    "    \"Would you recommend {MOVIE}?\",\n",
    "    # Comparisons\n",
    "    \"Which is better, {MOVIE} or {MOVIE}?\",\n",
    "    \"How does {MOVIE} compare to {MOVIE}?\",\n",
    "    # Behind the scenes\n",
    "    \"Are there any interesting facts about {MOVIE}?\",\n",
    "    \"Tell me some trivia about {MOVIE}.\",\n",
    "    # Soundtracks\n",
    "    \"Who composed the soundtrack for {MOVIE}?\",\n",
    "    \"Is the music in {MOVIE} noteworthy?\",\n",
    "    # Technical details\n",
    "    \"What camera was used to film {MOVIE}?\",\n",
    "    \"Was {MOVIE} shot in digital or on film?\",\n",
    "    # Language and subtitles\n",
    "    \"Is {MOVIE} in English?\",\n",
    "    \"Does {MOVIE} have subtitles?\",\n",
    "    # Cultural impact\n",
    "    \"How did {MOVIE} influence cinema?\",\n",
    "    \"What is the cultural significance of {MOVIE}?\",\n",
    "    # Audience\n",
    "    \"Is {MOVIE} suitable for children?\",\n",
    "    \"Can kids watch {MOVIE}?\",\n",
    "    # Plot specifics\n",
    "    \"Does {MOVIE} have a happy ending?\",\n",
    "    \"What happens at the end of {MOVIE}?\",\n",
    "    # Release formats\n",
    "    \"Is there a 3D version of {MOVIE}?\",\n",
    "    \"Was {MOVIE} released in IMAX?\",\n",
    "    # Miscellaneous\n",
    "    \"Did {MOVIE} face any controversies?\",\n",
    "    \"What are the themes explored in {MOVIE}?\",\n",
    "    \"Is {MOVIE} based on a book?\",\n",
    "    \"Who wrote the original story for {MOVIE}?\",\n",
    "    \"Are there any spin-offs from {MOVIE}?\",\n",
    "    \"What inspired the creation of {MOVIE}?\",\n",
    "]\n",
    "\n",
    "templates.extend(additional_templates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import csv\n",
    "import re\n",
    "\n",
    "DATASET_FILE = \"movie_title_dataset.json\"\n",
    "\n",
    "def process_template(template, selected_movies):\n",
    "    # Use regex to split the template by {MOVIE} placeholders\n",
    "    pattern = re.compile(r'(\\{MOVIE\\})')\n",
    "    parts = pattern.split(template)\n",
    "    sentence_words = []\n",
    "    labels = []\n",
    "    movie_idx = 0\n",
    "    for part in parts:\n",
    "        if part == \"{MOVIE}\":\n",
    "            # Replace with movie title and label it\n",
    "            movie = selected_movies[movie_idx]\n",
    "            movie_idx += 1\n",
    "            movie_words = movie.split()\n",
    "            sentence_words.extend(movie_words)\n",
    "            labels.extend([\"B-MOVIE\"] + [\"I-MOVIE\"] * (len(movie_words) - 1))\n",
    "        else:\n",
    "            # Split the non-placeholder parts into words and label as 'O'\n",
    "            words = part.split()\n",
    "            sentence_words.extend(words)\n",
    "            labels.extend([\"O\"] * len(words))\n",
    "    return sentence_words, labels\n",
    "\n",
    "def generate_dataset(movie_titles, templates, size=60000, output_file=DATASET_FILE):\n",
    "    dataset = []\n",
    "    SENTENCE_LEN_THRESHOLD = 100\n",
    "\n",
    "    # Step 1: Ensure each movie title is included at least once\n",
    "    for movie in movie_titles:\n",
    "        # Choose a template with at least one {MOVIE} placeholder\n",
    "        valid_templates = [t for t in templates if \"{MOVIE}\" in t]\n",
    "        template = random.choice(valid_templates)\n",
    "\n",
    "        # Count the number of {MOVIE} placeholders\n",
    "        movie_placeholders = template.count(\"{MOVIE}\")\n",
    "\n",
    "        # Select movies to replace placeholders\n",
    "        selected_movies = [movie]\n",
    "\n",
    "        # If more placeholders, fill with random movies excluding the current one\n",
    "        if movie_placeholders > 1:\n",
    "            remaining_movies = list(set(movie_titles) - set([movie]))\n",
    "            additional_movies = random.sample(remaining_movies, min(movie_placeholders - 1, len(remaining_movies)))\n",
    "            selected_movies.extend(additional_movies)\n",
    "\n",
    "        # Process the template to get words and labels\n",
    "        sentence_words, labels = process_template(template, selected_movies)\n",
    "\n",
    "        if len(sentence_words) > SENTENCE_LEN_THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        dataset.append({\"sentence\": sentence_words, \"labels\": labels})\n",
    "\n",
    "    # Step 2: Generate additional sentences to reach the desired size\n",
    "    while len(dataset) < size:\n",
    "        template = random.choice(templates)\n",
    "        movie_placeholders = template.count(\"{MOVIE}\")\n",
    "\n",
    "        # Randomly select movie titles for each placeholder\n",
    "        if movie_placeholders > 0:\n",
    "            selected_movies = random.sample(movie_titles, min(movie_placeholders, len(movie_titles)))\n",
    "        else:\n",
    "            selected_movies = []\n",
    "\n",
    "        # Process the template to get words and labels\n",
    "        sentence_words, labels = process_template(template, selected_movies)\n",
    "\n",
    "        if len(sentence_words) > SENTENCE_LEN_THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        dataset.append({\"sentence\": sentence_words, \"labels\": labels})\n",
    "\n",
    "    # Save dataset as JSON Lines\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for example in dataset:\n",
    "            f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "    # Convert the data to CSV for visual inspection\n",
    "    with open(\"movie_title_dataset.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"sentence\", \"labels\"])\n",
    "        for example in dataset:\n",
    "            # Convert lists to space-separated strings\n",
    "            sentence_str = ' '.join(example['sentence'])\n",
    "            labels_str = ' '.join(example['labels'])\n",
    "            writer.writerow([sentence_str, labels_str])\n",
    "\n",
    "    print(f\"Dataset saved to {output_file} and 'movie_title_dataset.csv'\")\n",
    "\n",
    "# Generate the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "63882378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = generate_dataset(movie_titles, templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1d591d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from JSON Lines file\n",
    "dataset = load_dataset('json', data_files=DATASET_FILE, split='train')\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_testvalid = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_testvalid['train']\n",
    "eval_dataset = test_valid['train']\n",
    "test_dataset = test_valid['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "tokenize the sentences in the dataset using the `BertTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "label_list = [\"O\", \"B-MOVIE\", \"I-MOVIE\"]\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_map[label[word_idx]])\n",
    "            else:\n",
    "                # Set the label for sub-tokens to -100 to ignore during loss computation\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10400004",
   "metadata": {},
   "source": [
    "Tokenization to all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "85527751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 9000/9000 [00:01<00:00, 6089.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenization to all datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Tune the BERT-base-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U accelerate\n",
    "# %pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should display your GPU name, e.g., 'NVIDIA GeForce RTX 4060'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f387785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "label_list = [\"O\", \"B-MOVIE\", \"I-MOVIE\"]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_list[pred] for pred, lab in zip(prediction, label) if lab != -100]\n",
    "                        for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wangy\\AppData\\Local\\Temp\\ipykernel_47316\\1216044110.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  1%|          | 82/7875 [11:25<18:05:25,  8.36s/it]\n",
      "  4%|▍         | 300/7875 [01:26<37:54,  3.33it/s]\n",
      "  4%|▍         | 300/7875 [01:26<37:54,  3.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0749, 'grad_norm': 0.022278793156147003, 'learning_rate': 1.923809523809524e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 600/7875 [02:52<31:16,  3.88it/s]\n",
      "  8%|▊         | 600/7875 [02:52<31:16,  3.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0082, 'grad_norm': 0.009743405506014824, 'learning_rate': 1.8476190476190478e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 900/7875 [04:18<36:10,  3.21it/s]\n",
      " 11%|█▏        | 900/7875 [04:18<36:10,  3.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0045, 'grad_norm': 0.003350650891661644, 'learning_rate': 1.7714285714285717e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1200/7875 [05:43<29:11,  3.81it/s]\n",
      " 15%|█▌        | 1200/7875 [05:43<29:11,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0039, 'grad_norm': 0.002397396368905902, 'learning_rate': 1.6952380952380955e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1500/7875 [07:01<27:12,  3.90it/s]\n",
      " 19%|█▉        | 1500/7875 [07:01<27:12,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0043, 'grad_norm': 0.019965114071965218, 'learning_rate': 1.6190476190476193e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1800/7875 [08:20<27:42,  3.65it/s]\n",
      " 23%|██▎       | 1801/7875 [08:20<26:58,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.003, 'grad_norm': 0.0021613319404423237, 'learning_rate': 1.542857142857143e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2100/7875 [09:37<24:49,  3.88it/s]\n",
      " 27%|██▋       | 2100/7875 [09:37<24:49,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0045, 'grad_norm': 0.16380150616168976, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 2400/7875 [10:55<23:28,  3.89it/s]\n",
      " 30%|███       | 2400/7875 [10:55<23:28,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0023, 'grad_norm': 0.0014672655379399657, 'learning_rate': 1.3904761904761905e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2625/7875 [11:53<22:06,  3.96it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                \n",
      "\n",
      "                                                   \n",
      " 33%|███▎      | 2625/7875 [12:40<22:06,  3.96it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0015026861801743507, 'eval_precision': 0.9970539133850536, 'eval_recall': 0.9984265906185466, 'eval_f1': 0.9977397798742139, 'eval_runtime': 46.368, 'eval_samples_per_second': 194.099, 'eval_steps_per_second': 12.142, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 2700/7875 [13:00<21:40,  3.98it/s]   \n",
      " 34%|███▍      | 2700/7875 [13:00<21:40,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'grad_norm': 0.0013948379782959819, 'learning_rate': 1.3142857142857145e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3000/7875 [14:19<25:48,  3.15it/s]\n",
      " 38%|███▊      | 3000/7875 [14:19<25:48,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'grad_norm': 0.0007511643343605101, 'learning_rate': 1.2380952380952383e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 3300/7875 [15:42<21:18,  3.58it/s]\n",
      " 42%|████▏     | 3300/7875 [15:42<21:18,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'grad_norm': 0.0686810240149498, 'learning_rate': 1.1619047619047621e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 3600/7875 [17:05<18:54,  3.77it/s]\n",
      " 46%|████▌     | 3600/7875 [17:05<18:54,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0019, 'grad_norm': 0.0007226763991639018, 'learning_rate': 1.0857142857142858e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 3900/7875 [18:23<18:20,  3.61it/s]\n",
      " 50%|████▉     | 3900/7875 [18:23<18:20,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'grad_norm': 0.0010616047075018287, 'learning_rate': 1.0095238095238096e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4200/7875 [19:50<17:44,  3.45it/s]\n",
      " 53%|█████▎    | 4200/7875 [19:50<17:44,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'grad_norm': 0.000839787011500448, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4500/7875 [21:15<15:57,  3.53it/s]\n",
      " 57%|█████▋    | 4500/7875 [21:15<15:57,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'grad_norm': 0.00047391006955876946, 'learning_rate': 8.571428571428571e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 4800/7875 [22:37<16:03,  3.19it/s]\n",
      " 61%|██████    | 4800/7875 [22:37<16:03,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'grad_norm': 0.00048770717694424093, 'learning_rate': 7.809523809523811e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 5100/7875 [24:05<13:51,  3.34it/s]\n",
      " 65%|██████▍   | 5100/7875 [24:05<13:51,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'grad_norm': 0.000417823379393667, 'learning_rate': 7.047619047619048e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 5250/7875 [24:49<11:27,  3.82it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                \n",
      "\n",
      "                                                   \n",
      " 67%|██████▋   | 5250/7875 [25:35<11:27,  3.82it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0009336563525721431, 'eval_precision': 0.9990169091624066, 'eval_recall': 0.9993116333956141, 'eval_f1': 0.9991642495452534, 'eval_runtime': 46.1107, 'eval_samples_per_second': 195.182, 'eval_steps_per_second': 12.21, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 5400/7875 [26:18<11:35,  3.56it/s]   \n",
      " 69%|██████▊   | 5400/7875 [26:18<11:35,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'grad_norm': 0.00039165583439171314, 'learning_rate': 6.285714285714286e-06, 'epoch': 2.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 5700/7875 [27:44<09:44,  3.72it/s]\n",
      " 72%|███████▏  | 5700/7875 [27:44<09:44,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'grad_norm': 0.00032071248278953135, 'learning_rate': 5.523809523809525e-06, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 6000/7875 [29:07<08:44,  3.57it/s]\n",
      " 76%|███████▌  | 6000/7875 [29:07<08:44,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'grad_norm': 0.0003315380890853703, 'learning_rate': 4.761904761904762e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 6300/7875 [30:25<07:05,  3.70it/s]\n",
      " 80%|████████  | 6300/7875 [30:25<07:05,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'grad_norm': 0.0003222887753508985, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 6600/7875 [31:43<05:22,  3.95it/s]\n",
      " 84%|████████▍ | 6601/7875 [31:43<05:19,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0002865996502805501, 'learning_rate': 3.2380952380952385e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 6900/7875 [33:00<04:07,  3.94it/s]\n",
      " 88%|████████▊ | 6900/7875 [33:01<04:07,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'grad_norm': 0.0002663623308762908, 'learning_rate': 2.4761904761904764e-06, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 7200/7875 [34:21<03:18,  3.40it/s]\n",
      " 91%|█████████▏| 7200/7875 [34:21<03:18,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'grad_norm': 0.0004077173362020403, 'learning_rate': 1.7142857142857145e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 7500/7875 [35:45<01:39,  3.78it/s]\n",
      " 95%|█████████▌| 7500/7875 [35:45<01:39,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'grad_norm': 0.0002916446828749031, 'learning_rate': 9.523809523809525e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 7800/7875 [37:04<00:19,  3.87it/s]\n",
      " 99%|█████████▉| 7801/7875 [37:04<00:18,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'grad_norm': 0.00028625200502574444, 'learning_rate': 1.904761904761905e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7875/7875 [37:23<00:00,  3.59it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                \n",
      "\n",
      "                                                   \n",
      "100%|██████████| 7875/7875 [38:10<00:00,  3.59it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006773102795705199, 'eval_precision': 0.9990169091624066, 'eval_recall': 0.9993116333956141, 'eval_f1': 0.9991642495452534, 'eval_runtime': 46.3061, 'eval_samples_per_second': 194.359, 'eval_steps_per_second': 12.158, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 7875/7875 [38:12<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2292.4771, 'train_samples_per_second': 54.962, 'train_steps_per_second': 3.435, 'train_loss': 0.004509789329909143, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7875, training_loss=0.004509789329909143, metrics={'train_runtime': 2292.4771, 'train_samples_per_second': 54.962, 'train_steps_per_second': 3.435, 'total_flos': 8230922251776000.0, 'train_loss': 0.004509789329909143, 'epoch': 3.0})"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=300,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_list[pred] for pred, lab in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693f20c",
   "metadata": {},
   "source": [
    "## Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "54405e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 563/563 [00:50<00:00, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0003611193969845772, 'eval_precision': 0.9997074882995319, 'eval_recall': 0.9998049731838128, 'eval_f1': 0.9997562283652675, 'eval_runtime': 51.1833, 'eval_samples_per_second': 175.839, 'eval_steps_per_second': 11.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to './fine_tuned_BERT_base_uncased'\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_BERT_base_uncased\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_BERT_base_uncased\")\n",
    "\n",
    "print(\"Model and tokenizer saved to './fine_tuned_BERT_base_uncased'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b2efd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \"Did Christopher Nolan direct Inception?\"\n",
      "  - Entity: 'inception', Label: 'LABEL_1', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Is GoldenEye 007 a James Bond movie?\"\n",
      "  - Entity: 'golden', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: '##eye 007', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Is Following a black and white film?\"\n",
      "  - Entity: 'following', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'and', Label: 'LABEL_2', Confidence: 0.74\n",
      "\n",
      "Sentence: \"Does the lord of the Rings Trilogy consist of three movies?\"\n",
      "  - Entity: 'the', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'lord of the rings trilogy', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Does First Man depict the life of Neil Armstrong?\"\n",
      "  - Entity: 'first', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'man', Label: 'LABEL_2', Confidence: 1.00\n",
      "  - Entity: 'of neil armstrong', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Is La Princesse de Clèves set in the French Renaissance period?\"\n",
      "  - Entity: 'la', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'princesse de cleves', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Is 2001: A Space Odyssey directed by Stanley Kubrick?\"\n",
      "  - Entity: '2001', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: ': a space odyssey', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Is Devil in the Flesh 2 a sequel?\"\n",
      "  - Entity: 'devil', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'in the flesh 2', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Did James Cameron direct Titanic?\"\n",
      "  - Entity: 'titanic', Label: 'LABEL_1', Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_BERT_base_uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./fine_tuned_BERT_base_uncased\")\n",
    "\n",
    "# Create the pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Example inference\n",
    "sentences = [\n",
    "    \"Is Wildcats directed by Stanley Kubrick?\",\n",
    "    \"Who played the lead role in Avatar\",\n",
    "    \"Who directed Night Across the Street\",\n",
    "    \"When was The Godfather released?\",\n",
    "    \"When was 'The Godfather' released?\",\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "]\n",
    "\n",
    "sentences_1 = [\n",
    "        \"Did Christopher Nolan direct Inception?\",\n",
    "    \"Is GoldenEye 007 a James Bond movie?\",\n",
    "\"Is Following a black and white film?\",\n",
    "\"Does the lord of the Rings Trilogy consist of three movies?\",\n",
    "\"Does First Man depict the life of Neil Armstrong?\",\n",
    "\"Is La Princesse de Clèves set in the French Renaissance period?\",\n",
    "\"Is 2001: A Space Odyssey directed by Stanley Kubrick?\",\n",
    "\"Is Devil in the Flesh 2 a sequel?\",\n",
    "\"Did James Cameron direct Titanic?\",\n",
    "]\n",
    "\n",
    "for s in sentences_1:\n",
    "    ner_results = ner_pipeline(s)\n",
    "\n",
    "    print(f\"\\nSentence: \\\"{s}\\\"\")\n",
    "    if ner_results:\n",
    "        for entity in ner_results:\n",
    "            label = entity[\"entity_group\"]\n",
    "            word = entity[\"word\"]\n",
    "            score = entity[\"score\"]\n",
    "            if label in ('LABEL_1', 'LABEL_2'):\n",
    "                print(f\"  - Entity: '{word}', Label: '{label}', Confidence: {score:.2f}\")\n",
    "    else:\n",
    "        print(\"No entities found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \"Let's talk about Avatar.\"\n",
      "  - Entity: 'Avatar', Label: 'LABEL_1', Confidence: 0.96\n",
      "\n",
      "Sentence: \"When was The Godfather released?\"\n",
      "  - Entity: 'The', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'Godfather', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"When was vampire assassin released?\"\n",
      "  - Entity: 'vampire', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'assassin', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Who is the screenwriter of The Masked Gang: Cyprus?\"\n",
      "  - Entity: 'The', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'Masked Gang :', Label: 'LABEL_2', Confidence: 1.00\n",
      "\n",
      "Sentence: \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\"\n",
      "  - Entity: 'Star', Label: 'LABEL_1', Confidence: 1.00\n",
      "  - Entity: 'Wars : Episode VI - Return of the', Label: 'LABEL_2', Confidence: 0.94\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./Tuned_BERT_NER_movie-60000\", tokenizer=\"./Tuned_BERT_NER_movie-60000\", aggregation_strategy=\"simple\", device=\"cuda\")\n",
    "\n",
    "# Example inference\n",
    "sentences = [\n",
    "    \"Let's talk about Avatar.\",\n",
    "    \"When was The Godfather released?\",\n",
    "    \"When was vampire assassin released?\",\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "]\n",
    "\n",
    "sentences_1 = [\n",
    "        \"Did Christopher Nolan direct Inception?\",\n",
    "    \"Is GoldenEye 007 a James Bond movie?\",\n",
    "\"Is Following a black and white film?\",\n",
    "\"Does the lord of the Rings Trilogy consist of three movies?\",\n",
    "\"Does First Man depict the life of Neil Armstrong?\",\n",
    "\"Is La Princesse de Clèves set in the French Renaissance period?\",\n",
    "\"Is 2001: A Space Odyssey directed by Stanley Kubrick?\",\n",
    "\"Is Devil in the Flesh 2 a sequel?\",\n",
    "\"Did James Cameron direct Titanic?\",\n",
    "]\n",
    "\n",
    "recommendation_sentence = [\n",
    "    \"Given that I like Inception, The Godfather can you recommend me some movies\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    ner_results = ner_pipeline(s)\n",
    "\n",
    "    print(f\"\\nSentence: \\\"{s}\\\"\")\n",
    "    if ner_results:\n",
    "        for entity in ner_results:\n",
    "            label = entity[\"entity_group\"]\n",
    "            word = entity[\"word\"]\n",
    "            score = entity[\"score\"]\n",
    "            if label in ('LABEL_1', 'LABEL_2'):\n",
    "                print(f\"  - Entity: '{word}', Label: '{label}', Confidence: {score:.2f}\")\n",
    "    else:\n",
    "        print(\"No entities found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATAIChatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
